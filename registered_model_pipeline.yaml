# PIPELINE DEFINITION  
# Name: simple-metrics-pipeline
# Description: KFP v2 pipeline that outputs proper metrics for UI testing
components:
  comp-generate-metrics:
    executorLabel: exec-generate-metrics
    inputDefinitions:
      parameters:
        model_name:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
      parameters:
        accuracy:
          parameterType: NUMBER_DOUBLE
        precision:
          parameterType: NUMBER_DOUBLE
        recall:
          parameterType: NUMBER_DOUBLE
        f1_score:
          parameterType: NUMBER_DOUBLE
deploymentSpec:
  executors:
    exec-generate-metrics:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - generate_metrics
        command:
        - sh
        - -c
        - |
          python3 -c "
          import kfp
          from kfp import dsl
          from kfp.dsl import *
          from typing import *
          import json
          import os
          
          def generate_metrics(
              model_name: str,
              metrics: Output[ClassificationMetrics],
              accuracy: Output[float],
              precision: Output[float], 
              recall: Output[float],
              f1_score: Output[float]
          ):
              print('Starting metrics generation...')
              
              # Log confusion matrix for classification metrics
              confusion_matrix = [
                  [85, 3, 2],   # True Positive, False Negative, False Negative  
                  [4, 88, 3],   # False Positive, True Positive, False Negative
                  [1, 2, 87]    # False Positive, False Positive, True Positive
              ]
              
              metrics.log_confusion_matrix(
                  ['Class_A', 'Class_B', 'Class_C'],
                  confusion_matrix
              )
              
              # Log scalar metrics
              metrics.log_metric('accuracy', 0.891)
              metrics.log_metric('precision', 0.903) 
              metrics.log_metric('recall', 0.887)
              metrics.log_metric('f1_score', 0.895)
              metrics.log_metric('training_accuracy', 0.924)
              metrics.log_metric('training_loss', 0.076)
              
              # Set output parameters
              accuracy.value = 0.891
              precision.value = 0.903
              recall.value = 0.887
              f1_score.value = 0.895
              
              print('Metrics logged successfully!')
              print(f'Accuracy: {0.891:.3f}')
              print(f'Precision: {0.903:.3f}')  
              print(f'Recall: {0.887:.3f}')
              print(f'F1 Score: {0.895:.3f}')
              
          # Execute the function with fake inputs
          class FakeOutput:
              def __init__(self):
                  self.value = None
                  
              def log_confusion_matrix(self, labels, matrix):
                  print(f'Confusion matrix logged with labels: {labels}')
                  
              def log_metric(self, name, value):
                  print(f'Metric logged: {name} = {value}')
          
          fake_metrics = FakeOutput()
          fake_accuracy = FakeOutput()
          fake_precision = FakeOutput()
          fake_recall = FakeOutput()
          fake_f1 = FakeOutput()
          
          generate_metrics('QuickTestModel', fake_metrics, fake_accuracy, fake_precision, fake_recall, fake_f1)
          "
        image: python:3.11-slim
pipelineInfo:
  name: simple-metrics-pipeline
root:
  dag:
    outputs:
      artifacts:
        pipeline-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: generate-metrics
    tasks:
      generate-metrics:
        cachingOptions:
          enableCache: false
        componentRef:
          name: comp-generate-metrics
        inputs:
          parameters:
            model_name:
              runtimeValue:
                constant: QuickTestModel
        taskInfo:
          name: generate-metrics
  inputDefinitions:
    parameters:
      model_name:
        defaultValue: QuickTestModel
        isOptional: true
        parameterType: STRING
  outputDefinitions:
    artifacts:
      pipeline-metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.9.0